import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common import TimeoutException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from urllib.parse import urlparse, urljoin

import time
import re

from text_processing.summarizer.text_summarizer import TextSummarizer


class FlexibleScraper:
    def __init__(self, url, scroll_infinite=False, min_content_size=500, max_depth=2):
        """
        Initialisez le scraper avec une URL du site, une option de d√©filement infini, ou le minimum de contenu,
        et une profondeur maximale pour rechercher d'autres pages sur mon site.
        """
        self.url = url
        self.scroll_infinite = scroll_infinite
        self.min_content_size = min_content_size
        self.max_depth = max_depth  # Define a profundidade m√°xima de p√°ginas a serem visitadas

        # Armazena os links j√° visitados para evitar scraping repetido
        self.visited_links = set()

    def _get_html(self):
        """
        Fonction pour obtenir le HTML de la page. Utilisez les requ√™tes pour les pages statiques et Selenium pour les pages dynamiques.
        """
        html = self._get_html_with_requests()

        if html and len(html) < self.min_content_size:
            print(f"Le contenu d√©tect√© est inf√©rieur {self.min_content_size}, tentative d'utilisation de Selenium...")
            return self._get_html_with_selenium()

        return html

    def _get_html_with_requests(self):
        """
        Obtient le code HTML √† l'aide des requ√™tes pour les pages statiques.
        """
        try:
            response = requests.get(self.url)
            if response.status_code == 200:
                return response.text
        except requests.exceptions.RequestException as e:
            print(f"Erreur lors de l'acc√®s √† la page avec les requ√™tes: {e}")
        return None

    def _get_html_with_selenium(self):
        """
        Obtient le HTML en utilisant Selenium (sans ouvrir la fen√™tre du navigateur).
        Peut inclure un d√©filement/scroll infini si n√©cessaire.
        """
        options = Options()
        options.add_argument('--headless')  # Rodar sem abrir a janela do navegador
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

        driver.get(self.url)

        try:
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
        except TimeoutException:
            print("Le temps d'attente d√©pass√©. La page ne s'est pas charg√©e comme pr√©vu.")
            driver.quit()
            return None

        time.sleep(2)  # Delai d'attente de 2 seconds pour assurer que la page se charge.

        if self.scroll_infinite:
            self._scroll_infinite(driver)

        html = driver.page_source
        driver.quit()
        return html

    def _scroll_infinite(self, driver):
        """
        Effectue un d√©filement infini sur la page.
        """
        last_height = driver.execute_script("return document.body.scrollHeight")

        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            WebDriverWait(driver, 10).until(
                lambda d: d.execute_script("return document.body.scrollHeight") > last_height)
            time.sleep(2)

            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height

    def _extract_text(self, soup):
        """
        Fun√ß√£o para extrair o texto da p√°gina.
        """
        return soup.get_text().strip()

    def _extract_internal_links(self, soup):
        """
        Fonction pour extraire le text de la page.
        """
        internal_links = set()
        base_url = urlparse(self.url).netloc
        for link in soup.find_all("a", href=True):
            href = link["href"]
            full_url = urljoin(self.url, href)
            # V√©rifie si le lien est interne au domaine
            if urlparse(full_url).netloc == base_url:
                internal_links.add(full_url)
        return internal_links

    def scrape_page(self, url, current_depth=0):
        """
        Fonction principale pour gratter une page, et √©galement rechercher des pages internes.
        """
        if current_depth > self.max_depth:
            return ""

        # Evita visitar a mesma p√°gina mais de uma vez
        if url in self.visited_links:
            return ""
        self.visited_links.add(url)

        print(f"Scraping/Grattage de pages: {url}")
        self.url = url  # Mettre √† jour l'URL de la nouvelle page
        html = self._get_html()

        if html:
            soup = BeautifulSoup(html, 'html.parser')
            content = self._extract_text(soup)

            # Extrait les liens internes et effectue un scraping r√©cursif
            internal_links = self._extract_internal_links(soup)
            for link in internal_links:
                if link not in self.visited_links:
                    content += "\n\n" + self.scrape_page(link, current_depth + 1)

            return content
        else:
            return "Impossible d'acc√©der √† la page."

    def remove_white_spaces(self, content):
        """
        Supprimer les espaces multiples entre les mots.
        """
        if content:
            content = re.sub(r"\n+", "\n", content)
            content = "\n".join(" ".join(line.split()) for line in content.splitlines())
        return content


    def scrape(self):
        """
        D√©marrez le processus de scraping √† l‚ÄôURL initiale et recherchez les pages internes.
        """
        print('[scraper.py, scrape()]')
        summ = TextSummarizer()
        orig_text = self.remove_white_spaces(self.scrape_page(self.url))
        
        summ_text = summ.summarize(orig_text)

        print('==================')
        print('[orig_text]: ', orig_text)
        print('-----------')
        print('[summ_text]: ', summ_text)
        print('-----------')

        return '[orig_text]: '+ orig_text + ' - [summ_text]: '+ summ_text

if __name__ == "__main__":
    url = "https://example.com"  # üîÅ Replace with a real test URL
    scraper = FlexibleScraper(url, scroll_infinite=False)
    result = scraper.scrape()
    print(result)

